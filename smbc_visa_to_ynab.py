#! /usr/bin/env python2.7

# The way this works:
#
# * log in via:		/vp/xt_login.do
# * hit the month via:	/vp/web_meisai/web_meisai_top.do
# * hit the CSV link:	/vp/dl/(web_)*meisai/(web_)*meisai_csv.do
#
# It's an interesting security model.  The userid/password just gets you in
# the front door; all actions after that must be accompanied by a one-time-only
# session number generated by every page hit and valid only for links on that
# page.  Nothing appears to be requiring POST here; it's all GET-okay.  The
# cookie is a use-it-or-lose-it affair, too, but that's easy enough to get
# around by repeatedly hitting web_meisai.
#
# It's also possible to grab all transactions past a certain month by appending
# a "1" to the requested date.  Feeding "-d 2015041" to the script will give
# everything SMBC has post-April 2015.

import urllib, urllib2, cookielib, re, sys, getopt, datetime
import unicodedata

site = 'https://www.smbc-card.com'
username = 'xxxxxx'
password = 'yyyyyy'

d = datetime.datetime.today()

# Tacking "1" onto the end gives us everything up until the end of time.
#csv_date = d.strftime('%Y%m') + "1"

opts, args = getopt.getopt(sys.argv[1:], "d:u:p:", ["date", "username", "password"])
for o, a in opts:
	if o in ("-d", "--date"):
		csv_date = a
	elif o in ("-u", "--username"):
		username = a
	elif o in ("-p", "--password"):
		password = p

cj = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

login_data = urllib.urlencode({'userid' : username, 'password' : password, 'strURL' : '' })
url = site + '/vp/xt_login.do'
resp = opener.open(url, login_data)
page = resp.read()

# We are now successfully logged in.

# Snarf the session ID from this page -- we need it to get to the monthly
# statement, and the CSV link has the (new) SN embedded in the URL.

p = re.compile('sn=[0-9a-z]*')
result = p.search(page)
full_sessionid = result.group()
sessionid = full_sessionid[3:]

url = site + '/vp/web_meisai/web_meisai_top.do'
login_data = urllib.urlencode({'sn' : sessionid, 'lang' : 'ja', 'p01' : csv_date})
resp = opener.open(url, login_data)
page = resp.read()

# Look for the CSV link.  It may or may not have a couple of "web_"s in there

p = re.compile('\/vp\/dl\/(web_)*meisai\/(web_)*meisai_csv\.do\?p01=[0-9]*\&amp\;sn=[0-9a-z]*')
result = p.search(page)

# How we parse this depends on whether it had a web_ in front, so we do it
# again just to flag
p = re.compile('\/vp\/dl\/web_meisai\/web_meisai_csv\.do\?p01=[0-9]*\&amp\;sn=[0-9a-z]*')
cleared = p.search(page)

url = site + result.group()
resp = opener.open(url)
page = resp.read()

# Translate it into UTF8.  We'll still have double-spaces, but we'll deal with
# that in a minute
page = page.decode('shift-jis').encode('utf-8')
page = unicodedata.normalize('NFKC', page.decode('utf8'))

# YNAB needs a CSV in this format:
# Date,Payee,Category,Memo,Outflow,Inflow
# (rows of data)
# ,,,,,

# ... but SMBC gives us data in two different formats, depending on whether
# it's cleared or not.  Case 1 (cleared) is:
#
# Date,Payee,Flow(+/-),# times,this time,Flow(+/-),(optional comment)
#
# Case 2 (not-cleared, still current month) is:
#
# Date,Payee,which card,#times (1),this time(if prev > 1),month this hits,amount,optional from currency,ISO From currency,optional rate, spot rate


print "Date,Payee,Category,Memo,Outflow,Inflow"

if cleared is None:
	for row in page.split('\n'):

		element = row.split(',')
		# skip EOF
                if len(element) < 2:
                  continue

		# all of this join/split nonsense is basically tr -s ' '
		date = ' '.join(element[0].split())
		payee = ' '.join(element[1].split())
		payee = payee.replace(',', ' ')
		memo = ' '.join(element[5].split())
		memo = memo.replace(',', ' ')
		flow = ' '.join(element[6].split())
		everything = date+','+payee+',,'+memo+','+flow+','
		print everything.encode('UTF-8')

else:
	working_date = '2015/01/01'	
	for row in page.split('\n'):
		#print "1 - row", row
		element = row.split(',')
		# skip EOF
		if len(element) < 2:
		  continue
		# Handle date first (if non-numeric then it's a cardowner
		# comment and we skip this line
		if (len(element[0]) > 0):
		  if element[0][0] != '2':
			#print "skipping cardowner comment"
			continue
		# If the the length of the first five columns are zero, then
		# this is a summary and we skip it
		if not len(element[0]) and not len(element[1]):
			#print "skipping summary"
			continue
		if (len(element[0]) > 0):
			working_date = element[0]

		date = working_date
		payee = ' '.join(element[1].split())
		payee = payee.replace(',', ' ')
		memo = ' '.join(element[6].rstrip().split())
		memo = memo.replace(',', ' ')
		flow = ' '.join(element[5].split())

		everything = date+','+payee+',,'+memo+','+flow+','
		print everything.encode('UTF-8')

print ",,,,,"
